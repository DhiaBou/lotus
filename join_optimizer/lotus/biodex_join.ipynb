{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "parquet_path = '/home/dhia/Downloads/0000.parquet'\n",
    "\n",
    "df = duckdb.query(f\"\"\"\n",
    "    SELECT abstract,\n",
    "--     concat('[',reactions, '] in this order of preference from most inportant to least important. Anything else that is not in this list is not relevant and should be discarded') as fulltext,\n",
    "    fulltext,\n",
    "    string_split(reactions, ',') AS reactions_list\n",
    "    FROM parquet_scan('{parquet_path}')\n",
    "    LIMIT 20\n",
    "\"\"\").to_df()\n",
    "\n",
    "df2 = (\n",
    "    df.explode('reactions_list')\n",
    "      .dropna(subset=['reactions_list'])\n",
    ")\n",
    "\n",
    "df2['reactions'] = df2['reactions_list'].str.strip()\n",
    "df2 = pd.DataFrame({'car_types': df2['reactions'].unique()[::-1]})\n",
    "\n",
    "df2"
   ],
   "id": "d1766453fee4269",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "import lotus\n",
    "from lotus.models import LM, SentenceTransformersRM\n",
    "from lotus.types import CascadeArgs\n",
    "from lotus.vector_store import FaissVS\n",
    "lm = LM(model=\"gpt-4o-mini\")\n",
    "rm = SentenceTransformersRM(model=\"intfloat/e5-base-v2\")\n",
    "vs = FaissVS()\n",
    "\n",
    "lotus.settings.configure(lm=lm, rm=rm, vs=vs)\n",
    "\n",
    "expr = \"Does {fulltext:left} precisely describe or report {car_types:right} ? It needs to be central, super relevant and not generic.\"\n",
    "\n",
    "\n",
    "cascade_args = CascadeArgs(recall_target=0.8, precision_target=0.8)\n",
    "res, stats = df.sem_join(df2, expr, cascade_args=cascade_args, return_stats=True)\n",
    "\n",
    "\n",
    "print(f\"Joined {df.shape[0]} rows from df1 with {df2.shape[0]} rows from df2\")\n",
    "print(f\"    Join cascade took {stats['join_resolved_by_large_model']} LM calls\")\n",
    "print(f\"    Helper resolved {stats['join_resolved_by_helper_model']} LM calls\")\n",
    "print(f\"Join cascade used {stats['total_LM_calls']} LM calls in total\")\n",
    "print(f\"Naive join would require {df.shape[0]*df2.shape[0]} LM calls\")\n",
    "print(res)\n"
   ],
   "id": "dd0d3f1ff9ed18ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# ————————————————\n",
    "# 1) Ground-truth mapping\n",
    "# original df has one row per abstract with a list of reactions\n",
    "true_map = (\n",
    "    df.set_index('fulltext')['reactions']\n",
    "      .apply(lambda lst: [r.strip() for r in lst])  # clean whitespace\n",
    "      .to_dict()\n",
    ")\n",
    "\n",
    "# ————————————————\n",
    "# 2) Predicted mapping\n",
    "# your `res` DataFrame has one row per joined (abstract, reaction)\n",
    "pred_map = (\n",
    "    res.groupby('fulltext')['reactions:right']\n",
    "       .apply(list)\n",
    "       .to_dict()\n",
    ")\n",
    "\n",
    "# ————————————————\n",
    "# 3) Align into parallel lists\n",
    "abstracts = list(true_map.keys())\n",
    "y_true = [ true_map[a] for a in abstracts ]\n",
    "y_pred = [ pred_map.get(a, []) for a in abstracts ]\n",
    "\n",
    "# ————————————————\n",
    "# 4) Binarize and compute metrics\n",
    "mlb    = MultiLabelBinarizer()\n",
    "Y_true = mlb.fit_transform(y_true)\n",
    "Y_pred = mlb.transform(y_pred)\n",
    "\n",
    "print(\"Micro-precision: \", precision_score(Y_true, Y_pred, average='micro'))\n",
    "print(\"Micro-recall:    \", recall_score(Y_true, Y_pred, average='micro'))\n",
    "print(\"Micro-F1:        \", f1_score(Y_true, Y_pred, average='micro'))\n",
    "print(\"Subset accuracy:\", accuracy_score(Y_true, Y_pred))\n"
   ],
   "id": "548a2f2f7a361730",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# — Debugging outputs —\n",
    "\n",
    "# Basic counts\n",
    "print(f\"Num abstracts: {len(abstracts)}\")\n",
    "print(f\"Total ground‐truth pairs: {sum(len(lbls) for lbls in y_true)}\")\n",
    "print(f\"Total predicted pairs:    {sum(len(lbls) for lbls in y_pred)}\\n\")\n",
    "\n",
    "# Peek at the first few examples\n",
    "for i, a in enumerate(abstracts[:3]):\n",
    "    print(f\"--- Abstract #{i} ---\")\n",
    "    print(a)\n",
    "    print(\"  True reactions:     \", true_map[a])\n",
    "    print(\"  Predicted reactions:\", pred_map.get(a, []), \"\\n\")\n",
    "\n",
    "# Build lists of false positives and false negatives\n",
    "false_positives = []\n",
    "false_negatives = []\n",
    "for a in abstracts:\n",
    "    true_set = set(true_map[a])\n",
    "    pred_set = set(pred_map.get(a, []))\n",
    "    for r in pred_set - true_set:\n",
    "        false_positives.append((a, r))\n",
    "    for r in true_set - pred_set:\n",
    "        false_negatives.append((a, r))\n",
    "\n",
    "# Print samples\n",
    "print(\"Sample false positives (predicted but not true):\")\n",
    "for abs_, rxn in false_positives[:10]:\n",
    "    print(f\"  • [{rxn}] in abstract: {abs_[:20]}...\")\n",
    "\n",
    "print(\"\\nSample false negatives (true but not predicted):\")\n",
    "for abs_, rxn in false_negatives[:10]:\n",
    "    print(f\"  • [{rxn}] in abstract: {abs_[:20]}...\")\n"
   ],
   "id": "88dd651869e4310",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for k, v in list(true_map.items())[:5]:\n",
    "    print(\"\\n \\n\\n \\n\")\n",
    "    print(\"---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "    print(\"---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "    print(\"---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "    print(k)\n",
    "\n",
    "    print(\"------->\", v)\n",
    "    print(\"--xx--->\", pred_map.get(k, []))"
   ],
   "id": "739752c6fe006eff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "res",
   "id": "d35342ea2cea34c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TOP k",
   "id": "55ad7435c286410a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ranked",
   "id": "56543d5896b154aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from lotus.types import ReasoningStrategy\n",
    "\n",
    "# -------------------------------------------\n",
    "# 1.  Grab the join output and normalise names\n",
    "# -------------------------------------------\n",
    "# If you asked for return_stats=True, the first element is the pairs DF\n",
    "pairs = res[0] if isinstance(res, tuple) else res\n",
    "lm = LM(model=\"gpt-4o-mini\")\n",
    "rm = SentenceTransformersRM(model=\"intfloat/e5-base-v2\")\n",
    "vs = FaissVS()\n",
    "\n",
    "lotus.settings.configure(lm=lm, rm=rm, vs=vs)\n",
    "# NB: after sem_join() LOTUS keeps the original column\n",
    "# names suffixed with “:left” / “:right”\n",
    "ART_COL = \"fulltext\"          # or whatever identifies the article rows\n",
    "LAB_COL = \"reactions\"        # the candidate reaction\n",
    "\n",
    "# -------------------------------------------\n",
    "# 2.  Add LOTUS rankingss (top-k per article)\n",
    "# -------------------------------------------\n",
    "rank_expr = (\n",
    "    \"Given {fulltext}, which {reactions} is most precisely relevant? \"\n",
    ")\n",
    "\n",
    "ranked, topk_stats = pairs.sem_topk(\n",
    "    rank_expr,\n",
    "    K = 5,                      # or 5 if you only need top-5\n",
    "    group_by=[ART_COL],        # rank within each article\n",
    "    method=\"quick\",\n",
    "    strategy = ReasoningStrategy.ZS_COT,\n",
    "\n",
    "    return_explanations = True,\n",
    "    return_stats=True\n",
    ")\n",
    "\n",
    "# -------------------------------------------\n",
    "# 3.  Build the gold-standard (exploded) label table\n",
    "# -------------------------------------------\n"
   ],
   "id": "ef1e4ad13f2407c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ranked\n",
    "gold = (\n",
    "    df[[\"fulltext\", \"reactions_list\"]]   # df is the original articles DF you loaded\n",
    "      .explode(\"reactions_list\")\n",
    "      .dropna(subset=[\"reactions_list\"])\n",
    "      .assign(reactions_list=lambda d: d[\"reactions_list\"].str.strip())\n",
    "      .rename(columns={\"fulltext\": ART_COL, \"reactions_list\": \"label_id\"})\n",
    ")\n",
    "\n",
    "# -------------------------------------------\n",
    "# 4.  Helper to compute RP@k exactly as in §5.2\n",
    "# -------------------------------------------\n",
    "def rp_at_k(pred_df, gold_df, k=5):\n",
    "    gold = gold_df.groupby(ART_COL)[\"label_id\"].apply(set).to_dict()\n",
    "\n",
    "    per_article = []\n",
    "    for art, true_set in gold.items():\n",
    "        preds = (pred_df.loc[pred_df[ART_COL] == art, LAB_COL]\n",
    "                          .head(k)\n",
    "                          .tolist())\n",
    "        hits = sum(p in true_set for p in preds)\n",
    "        denom = min(k, len(true_set))\n",
    "        per_article.append(hits / denom)\n",
    "\n",
    "    return sum(per_article) / len(per_article) if per_article else 0.0\n",
    "\n",
    "# -------------------------------------------\n",
    "# 5.  Evaluate\n",
    "# -------------------------------------------\n",
    "rp5  = rp_at_k(ranked, gold, k=5)\n",
    "rp10 = rp_at_k(ranked, gold, k=10)\n",
    "\n",
    "print(f\"RP@5  = {rp5:.3f}\")\n",
    "print(f\"RP@10 = {rp10:.3f}\")\n",
    "print(1)\n"
   ],
   "id": "8b2f21d0e6b6b45a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ranked.groupby(ART_COL).cumcount()",
   "id": "2e4739d72d71c86f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d27f2a74a10eb09d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
