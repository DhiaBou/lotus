{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e760e63f124f19b8",
   "metadata": {},
   "source": "# SEM INDEX\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T16:16:01.909725Z",
     "start_time": "2025-08-26T16:16:01.875423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import duckdb\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from lotus.dtype_extensions import ImageArray\n",
    "from lotus.types import CascadeArgs, ProxyModel\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "LOAD_INDEX = True\n",
    "\n",
    "FASHION_DATASET_DIR = os.getenv(\"FASHION_DATASET_DIR\")\n",
    "FASHION_PARQUET = os.path.join(FASHION_DATASET_DIR, \"styles.parquet\")\n",
    "FASHION_DETAILS_PARQUET = os.path.join(FASHION_DATASET_DIR, \"styles_details.parquet\")\n",
    "FASHION_IMAGES_DIR = os.path.join(FASHION_DATASET_DIR, \"images\")\n",
    "DATASET_CAPTION_DB_BLIP = os.path.join(FASHION_DATASET_DIR, \"fashion_dataset_caps_blip-image-captioning-large.db\")\n",
    "DATASET_CAPTION_DB_INSTRUCTBLIP = os.path.join(FASHION_DATASET_DIR, \"fashion_dataset_caps_instructblip-flan-t5-xl.db\")\n",
    "\n",
    "sample_size_percentage = 100\n",
    "seed = 80\n",
    "df = duckdb.query(f\"\"\"\n",
    "with images as (\n",
    "    SELECT *\n",
    "    FROM parquet_scan('{FASHION_PARQUET}')\n",
    "    USING SAMPLE {sample_size_percentage} PERCENT (reservoir, {seed})\n",
    "    )\n",
    "    select\n",
    "     images.id ,images.subcategory, images.articletype, images.basecolour, details.price, images.productDisplayName,\n",
    "     -- styleimages.default.resolutions.\"360X480\"  as imageURL\n",
    "     styleimages.default.imageURL  as imageURL\n",
    "    -- *\n",
    "    from images, parquet_scan('{FASHION_DETAILS_PARQUET}') details\n",
    "    where images.id = details.id\n",
    "    -- and details.price <1000\n",
    "    order by images.id\n",
    "\"\"\").to_df()\n",
    "\n",
    "df[\"image\"] = ImageArray(df[\"id\"].apply(lambda i: os.path.join(FASHION_IMAGES_DIR, f\"{int(i)}.jpg\")))\n",
    "df[\"image_url\"] = ImageArray(df[\"imageURL\"])\n",
    "\n",
    "\n"
   ],
   "id": "bd9b009019fd13ee",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Creating the index",
   "id": "fa1051743c8b80ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T16:16:02.976466Z",
     "start_time": "2025-08-26T16:16:01.932052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from lotus.fts_store.db_fts_store import SQLiteFTSStore\n",
    "from lotus.vector_store import FaissVS\n",
    "import lotus\n",
    "from lotus.models import LM, SentenceTransformersRM\n",
    "\n",
    "gpt_4o_mini = LM(\"gpt-4o-mini\")\n",
    "gpt_4o = LM(\"gpt-4o\")\n",
    "\n",
    "# CLIP embedding model – works for both text & image\n",
    "# rm  = SentenceTransformersRM(model=\"clip-ViT-B-32\")\n",
    "rm = SentenceTransformersRM(model=\"clip-ViT-L-14\", max_batch_size=32)\n",
    "\n",
    "lotus.settings.configure(lm=gpt_4o, helper_lm=gpt_4o_mini, rm=rm, vs=FaissVS(), cs=SQLiteFTSStore())"
   ],
   "id": "78fb4d3bf0f58ae",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 16:16:01,934 - INFO - Load pretrained SentenceTransformer: clip-ViT-L-14\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T16:16:03.030848Z",
     "start_time": "2025-08-26T16:16:03.028001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not LOAD_INDEX:\n",
    "    df = df.sem_index(\"image\", index_dir=f\"{FASHION_DATASET_DIR}/image_{sample_size_percentage}_index\")\n",
    "    df = df.sem_index(\"productDisplayName\",\n",
    "                      index_dir=f\"{FASHION_DATASET_DIR}/productDisplayName_{sample_size_percentage}_index\")\n",
    "\n"
   ],
   "id": "95d55c274350948c",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T16:16:03.082795Z",
     "start_time": "2025-08-26T16:16:03.079873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = df.load_sem_index(\"image\", index_dir=f\"{FASHION_DATASET_DIR}/image_{sample_size_percentage}_index\")\n",
    "df = df.load_sem_index(\"image_url\", index_dir=f\"{FASHION_DATASET_DIR}/image_{sample_size_percentage}_index\")\n",
    "df = df.load_sem_index(\"productDisplayName\",\n",
    "                       index_dir=f\"{FASHION_DATASET_DIR}/productDisplayName_{sample_size_percentage}_index\")"
   ],
   "id": "4b4592cea0da91d5",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "just take few samples for testing purposes",
   "id": "52a1001695a2a7b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T16:16:03.132017Z",
     "start_time": "2025-08-26T16:16:03.128641Z"
    }
   },
   "cell_type": "code",
   "source": "df = df.sample(n=1000, random_state=seed)",
   "id": "33afbc92512859af",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prompt",
   "id": "a6847d8826c66d6c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T16:16:03.181222Z",
     "start_time": "2025-08-26T16:16:03.179031Z"
    }
   },
   "cell_type": "code",
   "source": "prompt = \"dark formal clothes\"",
   "id": "58de5cf236a3929f",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Full LLM calls",
   "id": "b3ed2f3f83212c79"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T16:16:35.978164Z",
     "start_time": "2025-08-26T16:16:03.227271Z"
    }
   },
   "cell_type": "code",
   "source": "df_res_llm = df.sem_filter(prompt, col_li=[\"image_url\"], return_stats=False)\n",
   "id": "ab5b4f2d578adba8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering:   0%|           0/1000 LM calls [00:00<?, ?it/s]2025-08-26 16:16:04,008 - INFO - Retrying request to /chat/completions in 0.475701 seconds\n",
      "Filtering: 100%|██████████ 1000/1000 LM calls [00:32<00:00, 30.63it/s]\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Binary search filter",
   "id": "f04a103061f9897b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T16:17:00.538210Z",
     "start_time": "2025-08-26T16:16:36.028118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "cascade_args = CascadeArgs(\n",
    "    recall_target=0.9,\n",
    "    precision_target=0.9,\n",
    "    sampling_percentage=0.1,\n",
    "    proxy_model=ProxyModel.EMBEDDING_MODEL,\n",
    ")\n",
    "\n",
    "df_res_binary_s = df.sem_filter(prompt, col_li=[\"image_url\"], cascade_args=cascade_args,\n",
    "                                              return_stats=True, find_top_k=True)\n"
   ],
   "id": "b5f27a75d38cc16c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering: 100%|██████████ 1/1 LM calls [00:02<00:00,  2.67s/it]\n",
      "Filtering: 100%|██████████ 1/1 LM calls [00:00<00:00,  1.10it/s]\n",
      "Filtering: 100%|██████████ 1/1 LM calls [00:01<00:00,  1.97s/it]\n",
      "Filtering: 100%|██████████ 1/1 LM calls [00:01<00:00,  1.04s/it]\n",
      "Filtering: 100%|██████████ 1/1 LM calls [00:02<00:00,  2.96s/it]\n",
      "Filtering: 100%|██████████ 1/1 LM calls [00:02<00:00,  2.00s/it]\n",
      "Filtering: 100%|██████████ 1/1 LM calls [00:02<00:00,  2.73s/it]\n",
      "Filtering: 100%|██████████ 1/1 LM calls [00:02<00:00,  2.13s/it]\n",
      "Filtering: 100%|██████████ 1/1 LM calls [00:02<00:00,  2.54s/it]\n",
      "Filtering: 100%|██████████ 1/1 LM calls [00:02<00:00,  2.16s/it]\n",
      "Filtering: 100%|██████████ 1/1 LM calls [00:01<00:00,  1.45s/it]\n",
      "Filtering: 100%|██████████ 1/1 LM calls [00:01<00:00,  1.77s/it]\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "from join_optimizer.lotus.evaluate import evaluate_filter\n",
    "\n",
    "metrics, FP, FN = evaluate_filter(\n",
    "    dataset_df=df_res_llm,\n",
    "    filtered_df=df_res_binary_s,\n",
    ")\n",
    "print(metrics)"
   ],
   "id": "79e2c51db3296a4d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Sampling",
   "id": "51da7aa32b23c1fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cascade_args = CascadeArgs(\n",
    "    recall_target=0.95,\n",
    "    precision_target=0.9,\n",
    "    sampling_percentage=0.1,\n",
    "    proxy_model=ProxyModel.EMBEDDING_MODEL,\n",
    "    cascade_IS_weight=1,\n",
    "    cascade_num_calibration_quantiles=100,\n",
    "    failure_probability=0.1,\n",
    "    cascade_IS_random_seed=114,\n",
    ")\n",
    "\n",
    "df_res_lotus = df.sem_filter(prompt, col_li=[\"image_url\"], cascade_args=cascade_args, return_stats=False,\n",
    "                                           find_top_k=False)\n"
   ],
   "id": "29988d7e8c718e29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "metrics, FP, FN = evaluate_filter(\n",
    "    dataset_df=df_res_llm,\n",
    "    filtered_df=df_res_lotus,\n",
    ")\n",
    "print(metrics)"
   ],
   "id": "408c24071c23b69a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Caption Search BLIP\n",
   "id": "9950937e86bdc1be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = df.sem_captions_index.attach_index(\"image\", index_dir=DATASET_CAPTION_DB_BLIP)\n",
    "df = df.sem_captions_index.load(\"image\")\n",
    "df_res_blip = df.sem_captions_index.search(prompt, \"image\")"
   ],
   "id": "2c46f418b5fa8dc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "metrics, FP, FN = evaluate_filter(\n",
    "    dataset_df=df_res_llm,\n",
    "    filtered_df=df_res_blip,\n",
    ")\n",
    "print(metrics)\n"
   ],
   "id": "b32cd061bbe6cdda"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### with prompt augmentation",
   "id": "6f0e65465410a71"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def augment_prompt(prompt, augmentation_prompt):\n",
    "    prompt_as_df = pd.DataFrame({\"query\": [prompt]})\n",
    "    return prompt_as_df.sem_map(augmentation_prompt, suffix=\"augmented_prompt\")[\"augmented_prompt\"][0].replace(\"'\", \" \").replace(\"-\", \" \")\n"
   ],
   "id": "8794f34b35e894ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prompt_augmentation_prompt = \"you will receive a {prompt} to do a full text search filter on a dataset. since the search is sintactical, provide 10 other prompts similar to the one provided, so that similar items can be obtained. Separate the results with a simple space and without delimiters like \\\" or \\«. only respond with the result.\"\n",
    "\n",
    "prompt_augmentation_prompt = \"\"\"\n",
    "You will receive a plain-language search {query} and must return a SINGLE valid SQLite FTS5 MATCH expression (the right-hand side of `... MATCH <expr>`). Return ONLY the expression, with no quotes around the whole thing, no SQL, no code fences, and no explanations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "REQUIREMENTS\n",
    "1) Output must be valid FTS5 boolean syntax using ONLY: parentheses `()`, `AND`, `OR`, `NOT`, double-quoted phrases, and (optionally) `NEAR` when allowed below. Do NOT use field qualifiers, weights, or other SQL.\n",
    "2) Group synonyms/near-lexicon with OR inside parentheses. Use AND between concept buckets.\n",
    "   - Example shape: `(concept1_a OR concept1_b OR \"concept1 phrase\") AND (concept2_a OR concept2_b) ...`\n",
    "3) Expand the user_query into 2–5 concept buckets (meaningful facets like style, color/tone, item types, descriptors, etc.). Inside each bucket, include common synonyms, close lexical variants, and singular/plural irregulars. The database already handles case, diacritics, and stemming—only add explicit variants when helpful (e.g., \"tuxedo OR tuxedos\", \"black-tie OR \\\"black tie\\\"\").\n",
    "4) If must_include is provided, ensure each term/phrase is present by adding extra AND groups for them (quoted as needed).\n",
    "5) If exclude is provided, append `AND NOT (...)` with OR-joined terms/phrases to filter them out.\n",
    "6) Phrases must use double quotes (e.g., \"black tie\"). Do NOT wrap the entire output in quotes.\n",
    "7) Avoid `*` wildcards unless the input explicitly asks for prefix search.\n",
    "8) Proximity:\n",
    "   - If require_proximity = true, use NEAR **only in properly nested binary form** and at most to tie TWO buckets: `((bucketA) NEAR (bucketB)) AND (bucketC) ...`. Never chain `A NEAR B NEAR C` without nesting.\n",
    "   - If require_proximity = false (default), do NOT use NEAR.\n",
    "9) Keep the expression concise (<1000 characters).\n",
    "\n",
    "OUTPUT\n",
    "- Only the MATCH expression, For example:\n",
    "  `(elegant OR upscale OR formal) AND (\"black tie\" OR \"black-tie\" OR dressy) AND (suit OR suits OR tuxedo OR tuxedos OR dress OR dresses OR gown OR gowns OR tie OR ties OR \"bow tie\" OR \"bow-tie\" OR blazer OR blazers OR jacket OR jackets OR trousers OR pants OR slacks OR skirt OR skirts) AND (dark OR black OR charcoal OR onyx OR midnight OR navy OR indigo OR burgundy OR maroon)`\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "augmented_prompt = augment_prompt(prompt, prompt_augmentation_prompt)\n",
    "print(augmented_prompt)"
   ],
   "id": "b328bb310f51168f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_res_blip_augmented = df.sem_captions_index.search(augmented_prompt, \"image\")\n",
   "id": "303e168f0b0929a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "metrics, FP, FN = evaluate_filter(\n",
    "    dataset_df=df_res_llm,\n",
    "    filtered_df=df_res_blip,\n",
    ")\n",
    "print(metrics)\n"
   ],
   "id": "989c18e9067db3c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Caption Search INSTRUCTBLIP",
   "id": "18ebca4f6514698a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = df.sem_captions_index.attach_index(\"image\", index_dir=DATASET_CAPTION_DB_INSTRUCTBLIP)\n",
    "df = df.sem_captions_index.load(\"image\")\n",
    "df_res_instructblip = df.sem_captions_index.search(prompt, \"image\")"
   ],
   "id": "827ecf3cd4e27b61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "metrics, FP, FN = evaluate_filter(\n",
    "    dataset_df=df_res_llm,\n",
    "    filtered_df=df_res_instructblip,\n",
    ")\n",
    "print(metrics)\n"
   ],
   "id": "a4bc82778171481d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Augmented",
   "id": "accb46fc7387825e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_res_instructblip_augmented = df.sem_captions_index.search(augmented_prompt, \"image\")",
   "id": "a0f224659d8b8f8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "metrics, FP, FN = evaluate_filter(\n",
    "    dataset_df=df_res_llm,\n",
    "    filtered_df=df_res_instructblip_augmented,\n",
    ")\n",
    "print(metrics)\n"
   ],
   "id": "416703674bc00730"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Pass through small model",
   "id": "54267a36441f3494"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T16:17:34.872955Z",
     "start_time": "2025-08-26T16:17:13.923258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_res_captions_small_model = df.sem_filter(\n",
    "    prompt, col_li=[\"image_cap\"])"
   ],
   "id": "38e014914ec4a60e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering: 100%|██████████ 1000/1000 LM calls [00:20<00:00, 47.99it/s]\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T16:17:34.928453Z",
     "start_time": "2025-08-26T16:17:34.924135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "metrics, FP, FN = evaluate_filter(\n",
    "    dataset_df=df_res_llm,\n",
    "    filtered_df=df_res_captions_small_model,\n",
    ")\n",
    "print(metrics)\n"
   ],
   "id": "178fd475ded0edaf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TP': 11, 'FP': 3, 'FN': 24, 'precision': 0.7857142857142857, 'recall': 0.3142857142857143, 'f1': 0.44897959183673464}\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### FN example\n",
    "\n",
    "\"The image features a woman wearing a green dress and a black dress. She is posing in front of a white background, with her hands placed on her hips. The woman is holding a handbag in her right hand, while her left hand is positioned closer to the camera. The handbag can be seen\"\n",
    "\n",
    "dark formal clothes ? yes or no\n",
    "\n",
    "Answer:\n",
    "\n",
    "No — the description does not suggest **dark formal clothes**.\n",
    "\n",
    "* The **green dress** is not typically considered dark formal attire.\n",
    "* The mention of a **black dress** could be, but since the description is ambiguous (she is said to be wearing *both* a green and a black dress), it’s unclear whether the outfit is actually formal.\n",
    "* The **context** (posing with a handbag, hands on hips, white background) suggests more of a **fashion photo shoot** rather than formal wear.\n",
    "\n",
    "✅ So the answer is **No** — this does not clearly depict dark formal clothes.\n",
    "\n",
    "http://assets.myntassets.com/v1/images/style/properties/Femella-Women-Green---Black-Dress_599cf4fdf7051ee5aed2629240ecdc5b_images.jpg"
   ],
   "id": "ad64a78725054293"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lotus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
